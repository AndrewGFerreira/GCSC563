---
title: "Email Classification Using Natural Language Processing "
author: "Andrew Ferreira, Michael VanDusen"
date: "1/21/2022"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

## Downloading the dataset
Feel free to [download](https://www.yelp.com/dataset/documentation/main) the dataset from yelp. 
It is a zip file with many datasets, including the review dataset, a 6GB json file.

This file is too large for many laptops to handle. The code below helps to extract a subset to move foward on this project.

```{r Install library, include=FALSE}
library(dplyr)
library(tidyverse) 
library(tidytext)
library(ggplot2)
library(forcats)
library(igraph)
library(ggraph)
```


```{r Extracting subset, eval=FALSE}

# It opens the json file
con_in <- file("~/review_dataset.json") 

# It will create and open a new file to store the subset
con_out <- file(tmp <- tempfile("subset", fileext = ".json"), open = "wb")

# The stream_in function will split the file into pages with 5000 records 
# (as indicated), and it will find records where there is at least one vote for 
# 'useful', 'funny' or 'cool' and it will store those records in the new file.
stream_in(con_in, handler = function(df)
  {
    df <- dplyr::filter(df, useful > 0 | funny > 0 | cool > 0)
    stream_out(df, con_out, pagesize = 5000)
  }, pagesize = 5000)

# The code above will run until the end of the primary file, or until you 
# manually stop it. For the puporse of this project, I stopped the code when 
# it reached 2MM records. The code below will save and close the temporary file. 
# Without this step, the temporary file will remain opened and without any 
# records.
close(con_out)

# The code below will give you the directory of the new file, so you can rename 
# it (like subset.json) and use it later.
tempdir()

# Load the subset file into a data frame
yelpReviews <- stream_in(file("~/subset.json"), pagesize = 10000)


```

![review.json](/Users/andrew.ferreira/Library/Mobile Documents/com~apple~CloudDocs/Documents/ORAL ROBERTS UNIVERSITY/MSCS/Artificial Intelligence/Project/GroupProject/GCSC563/Screen Shot 2022-02-02 at 7.55.37 PM.png)
## Cleaning and transforming the data

The original dataset is structered in a way that does not fit for the purpose
of this project. That's why we need to clean and transform some of it. 

```{r Cleaning and transforming the data, eval=FALSE}

# Create a subset to preserve the original dataset
subset_yelpReviews <- yelpReviews

# Select only necessary columns
dataset <- short_yelpReviews %>%
  select(review_id, text, useful, funny, cool)

# Transpose data and transform the 'category' columns into factor
dataset <- gather(data = dataset, key="category", value="votes", useful, funny, cool)
dataset$category <- as.factor(dataset$category)

# Partion the data by review_id, and rank votes for each review, where 3 is
# the most voted category
library(plyr) #to run ddply ()
dataset_sorted <- ddply(dataset,.(review_id),transform,order = rank(votes,ties.method = "first"))
detach("package:plyr", unload = TRUE) #to avoid future conflicts with dplyr

# Select only the most voted category
dataset_sorted <- dataset_sorted %>%
  filter(order == 3)

# Select only necessary columns
reviewsDataset <- dataset_sorted %>%
  select(text, category)

```

## Exploratory Data Analysis (EDA) - Part one: unigram

The "bag-of-words" model will try to use the word frequency to determine a category of text, while the n-gram will try to determine the category by analyzing a group of n words in a text.

Here is how the book describes:

> _the word "quarter" might be very frequent in both the business and sports category, but the four-word sequence "first quarter earnings report" is only common in business while "fourth quarter touchdown passes" in sports._ (Chapter 23 - Natural Language Processing)

Our objective during the EDA is to find out is to determine what model might be a better fit for text categorization.

OBS:After both steps above, a .csv file was created, but the file is too large (755MB) via R markdown. 
Please,  [click here to download](https://www.dropbox.com/s/r0ckgo9v21b5wp5/yelp_dataset%20dataset.csv?dl=0) the file and run this analysis. 

```{r Load file}

# Initialize dataset (paste the path to the file in your computer)
data <- read.csv("/Users/andrew.ferreira/Dropbox (Life.Church)/Mac/Downloads/yelp_dataset/yelp_dataset dataset.csv")
# uncomment if you want to run a sample data <- sample_n(data, 15000)

#Select only the necessary columns
dataset <- data %>%
  select(text, category)

dataset$category <- as.factor(dataset$category)

head(dataset,10)
```

The process of dividing a text into a sequence of words (either 1 or n) is called tokenization.
The code below will split the texts into words, and count the frequency of each of them.

```{r tokenization n1}

# Separate and count the frequency each word
datasetWords <- dataset %>%
  unnest_tokens(word, text) %>%
  count(category, word, sort = TRUE)

# Count the number of words in each category
datasetTotalWords <- datasetWords %>%
  group_by(category) %>%
  summarize(total = sum(n))

# Show the number of times each word appears in each category in contrast 
# with the amount of words in that category
categoryWords <- left_join(datasetWords, datasetTotalWords)
head(categoryWords, 20)

```
At this point we can see, just by looking at the top 20, that vague words like "the," "and," "for," are very common.
Let's take a look at the word frequency distribution in each category

```{r word frequency distribution}

ggplot(categoryWords, aes(n/total, fill = category)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~category, ncol = 2, scales = "free_y")
```

It seems like the frequency of those vague and common words are ridiculously high skewing our data. Another possible explanation to this chart is some very rare words at the end.

Let's rank the appearance of each word in each category where the most frequent word is the number 1.

```{r Ranking each word}
# Rank each word based on frequency
freq_by_rank <- categoryWords %>% 
  group_by(category) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

# Show top 20
head(freq_by_rank, 20)

# Rank x term Frequency
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = category)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) 

# Rank x term Frequency (log-log coordinates)
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = category)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

The chart above is a very import chart, because we can clearly see that the frequency of words are very similar in the three categories (cool, funny, and useful). This is a strong indicator that the "bag-of-words" might not be a good fit for us. 

However, at the end of each line we can see a distinction. Maybe the frequency of some rare words in each category might be strong enough to predict the category. 

Let's create linear model based on word frequency, just to confirm what we discovered so far.

```{r Linear model word frequency}
# Let's filter by rank, eliminating the top 10 words (the, of, and, etc.) and 
# including up to 5000
rank_subset <- freq_by_rank %>% 
  filter(rank < 5000,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
# Outcome: Intercept: -0.1981 Slope: -1.2771 

# I manually copy and past the intercept and slope
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = category)) + 
  geom_abline(intercept = -0.1981, slope = -1.2771, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

As we can see, the frequency of a word in our dataset does not seem to fit very well the linear model. 

There is a function in the tidytext package that will help us to decrease the weight of super common words, and increase the weight of words that are not super frequent. 

Julia Silge and David Robinson will say

> Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. (Source: Text Mining with R)

```{r Calculating tf idf}
# Call function for each category
category_tf_idf <- categoryWords %>%
  bind_tf_idf(word, category, n)

# Print top 20
head(category_tf_idf, 20)
# We can see that the tf_idf for vague and common words are very close to 0, 
# and the idf for them is 0 because those words appears in the three categories 
# we are looking at

# We can arrange the table to show the words that are more specific and frequent 
# in each category
category_tf_idf <- category_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))

# Let's look the top 20
unigram_tf_idf <- category_tf_idf %>%
  group_by(category) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

head(category_tf_idf, 20)
unigram_tf_idf
```


The charts above is showing us the 20 most strong words of each category. 
While words like 'daycares' or 'recruiters' kind of make sense for useful reviews, it is hard to find a "funny" or "cool"
word in the top 20. Maybe now it is a good time to start looking at a group of words, or n-gram.

## Exploratory Data Analysis (EDA) - Part two: bigram and trigram

Let's see if capturing the interaction between two (bigram) and three (trigram) brings more meaning to those top words.
It might still be vague because of the size of our dataset, but this might help us decide between the bag-of-words model and the ngram model.
If the EDA do not answer our question, we might jump straight to modeling the data and analysing accuracy or we can try smoothing the n-gram.

```{r Prepare ngram dataset}
set.seed(2022)

# Getting a sample to calculate tri and bi grams
dataset_bi_sample <- sample_frac(dataset, .10)
dataset_tri_sample <- sample_frac(dataset, .10)  

# preparing bigram dataset
dataset_bi_sample <-  dataset_bi_sample %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dataset_bi_count <- dataset_bi_sample %>%
  count(bigram, sort = TRUE)

# preparing trigram dataset
dataset_tri_sample <-  dataset_tri_sample %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

dataset_tri_count <- dataset_tri_sample %>%
  count(trigram, sort = TRUE)

# print results
head(dataset_bi_sample, 10)
head(dataset_bi_count, 10)

head(dataset_tri_count, 10)
head(dataset_tri_sample, 10)
```

The trigram is too large to compute and bigram was taking too long, so I had to create a sample dataset of only 10% of the original dataset and 10% for the bigram. With both datasets we can see that by looking ngram, the top 20 is more meaningful. 

Still, authors might disagree on the next step at this point. For an EDA perspective, it might be helpful to split the words from the bi and tri grams and create a list of 'stop-words' (words super common such as 'the', 'of') and remove those words from the ngrams. By doing this, we might reduce the sckew of our histogram, and get a better distribution. But from ML perspective, we don't want to hard code anything, since the very nature of ML is to avoid hard-coding the model.

Still, both points-of-view agree that removing some 'insignificant' words might be helpful. 
While EDA authors recommend hard code a list of stop-words, ML author recomend setting a frequency threshold (exclude words with frequency less than < 0.0001%)

```{r Separating the words}
set.seed(2022)

# Reducing the dataset
dataset_bi_sample_separated <- sample_frac(dataset_bi_sample, .10)
dataset_tri_sample_separated <- sample_frac(dataset_tri_sample, .10)  

# separating thr words
bigram_separated <- dataset_bi_sample_separated %>%
  separate(bigram, c("word1", "word2"), sep = " ")

trigram_separated <- dataset_tri_sample_separated %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

# shows results
head(bigram_separated, 10)
head(trigram_separated, 10)

```

Now that the words are separated, we can try to remove the 'stop-words', which R in fact already have the list for us

```{r Remove stop-words}

# remove stop-words
bigrams_filtered <- bigram_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
  
trigrams_filtered <- trigram_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

# sort the table
bigram_counts_separated <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

trigram_counts_separated <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

#show results
head(bigram_counts_separated,10)
head(trigram_counts_separated,10)
```

This gives us a very nice result. Before removing the stop-words, the top 10 bigram was 'of the' or 'it was',and the trigram was 'and i had' or 'ago and i'
but after removing the stop word, we got 'customer service' or 'highly recommend' for bigram and 'poor customer service' or 'gluten free options' for trigram.

```{r Unite filtered data}

# unite words again
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

# show results
head(bigrams_united, 10)
head(trigrams_united, 10)

```

We can also apply the tf_idf analysis to find out what ngrams are more specific for each category

```{r tf_idf for ngrams}

# Calculate tf_idf
bigram_tf_idf <- bigrams_united %>%
  count(category, bigram) %>%
  bind_tf_idf(bigram, category, n) %>%
  arrange(desc(tf_idf))

trigram_tf_idf <- trigrams_united %>%
  count(category, trigram) %>%
  bind_tf_idf(trigram, category, n) %>%
  arrange(desc(tf_idf))

# Represent graphically
bigram_tf_idf_chart <- bigram_tf_idf %>%
  group_by(category) %>%
  slice_max(tf_idf, n = 8) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

trigram_tf_idf_chart <- trigram_tf_idf %>%
  group_by(category) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(trigram, tf_idf), fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

# Show results
head(bigram_tf_idf, 10)
bigram_tf_idf_chart
head(trigram_tf_idf, 10)
trigram_tf_idf_chart
```
The final result is not great, but it is better than the bag-of-words analysis. using the ngram we can see more meaningful results, but it increases the computation.

```{r Analysing nodes}
# represent them graphically
bigram_graph <- bigram_counts_separated %>%
  filter(n > 40) %>%
  graph_from_data_frame()

trigram_graph <- trigram_counts_separated %>%
  filter(n > 5) %>%
  graph_from_data_frame()

bigram_graph_nodes <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

trigram_graph_nodes <- ggraph(trigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

bigram_graph_nodes
trigram_graph_nodes
```


The chart above helps us to visualize words with multiple connections. For example 'customer' is a key word with many ramifications, and numbers such as '1' can also lead to multiple branches. 

```{r Analysing nodes with direction}

a <- grid::arrow(type = "closed", length = unit(.08, "inches"))

bigram_graph_direction <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

trigram_graph_direction <- ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

bigram_graph_direction
trigram_graph_direction
```


Now can see the direction of the words and understand their relationship

next steps:
 1. Verify if we can and if we need to filter companies only in the US
 2. Maybe expand the samples to more than 10%
 3. Consider smoothing the ngram (in the book)
 4. play with numbers on the bigram_graph_direction and trigram_graph_direction to find a better representation
 
So far, I would say to pick bigram because it is more meaningful than bag-of-words and not too large to compute as the trigram
